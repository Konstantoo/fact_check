import re
from typing import Dict, List, Tuple
from urllib.parse import urlparse

class SourceValidatorService:
    """Ð¡ÐµÑ€Ð²Ð¸Ñ Ð´Ð»Ñ Ð²Ð°Ð»Ð¸Ð´Ð°Ñ†Ð¸Ð¸ Ð¸ Ñ€Ð°Ð½Ð¶Ð¸Ñ€Ð¾Ð²Ð°Ð½Ð¸Ñ Ð¸ÑÑ‚Ð¾Ñ‡Ð½Ð¸ÐºÐ¾Ð² Ð¿Ð¾ Ð½Ð°Ð´ÐµÐ¶Ð½Ð¾ÑÑ‚Ð¸"""
    
    def __init__(self):
        # Ð”Ð¾Ð¼ÐµÐ½Ñ‹ Ñ Ð²Ñ‹ÑÐ¾ÐºÐ¾Ð¹ Ð½Ð°Ð´ÐµÐ¶Ð½Ð¾ÑÑ‚ÑŒÑŽ (Ð½Ð°ÑƒÑ‡Ð½Ñ‹Ðµ, Ð¾Ñ„Ð¸Ñ†Ð¸Ð°Ð»ÑŒÐ½Ñ‹Ðµ, Ð¼ÐµÐ¶Ð´ÑƒÐ½Ð°Ñ€Ð¾Ð´Ð½Ñ‹Ðµ)
        self.high_reliability_domains = {
            # ÐÐ°ÑƒÑ‡Ð½Ñ‹Ðµ Ð¶ÑƒÑ€Ð½Ð°Ð»Ñ‹ Ð¸ Ð±Ð°Ð·Ñ‹ Ð´Ð°Ð½Ð½Ñ‹Ñ…
            'nature.com', 'science.org', 'pubmed.ncbi.nlm.nih.gov', 'scholar.google.com',
            'arxiv.org', 'jstor.org', 'springer.com', 'wiley.com', 'elsevier.com',
            
            # ÐžÑ„Ð¸Ñ†Ð¸Ð°Ð»ÑŒÐ½Ñ‹Ðµ Ð¼ÐµÐ¶Ð´ÑƒÐ½Ð°Ñ€Ð¾Ð´Ð½Ñ‹Ðµ Ð¾Ñ€Ð³Ð°Ð½Ð¸Ð·Ð°Ñ†Ð¸Ð¸
            'un.org', 'who.int', 'imf.org', 'worldbank.org', 'oecd.org', 'europa.eu',
            'fda.gov', 'cdc.gov', 'nih.gov', 'nsa.gov', 'cia.gov',
            
            # ÐœÐµÐ¶Ð´ÑƒÐ½Ð°Ñ€Ð¾Ð´Ð½Ñ‹Ðµ Ð½Ð¾Ð²Ð¾ÑÑ‚Ð½Ñ‹Ðµ Ð°Ð³ÐµÐ½Ñ‚ÑÑ‚Ð²Ð°
            'reuters.com', 'ap.org', 'afp.com', 'bbc.com', 'dw.com', 'france24.com',
            'aljazeera.com', 'dw.com', 'rt.com', 'sputniknews.com',
            
            # ÐÐºÐ°Ð´ÐµÐ¼Ð¸Ñ‡ÐµÑÐºÐ¸Ðµ Ð¸Ð½ÑÑ‚Ð¸Ñ‚ÑƒÑ‚Ñ‹
            'harvard.edu', 'mit.edu', 'stanford.edu', 'yale.edu', 'princeton.edu',
            'oxford.ac.uk', 'cambridge.ac.uk', 'sorbonne.fr', 'mpg.de',
            
            # ÐŸÑ€Ð¾Ñ„ÐµÑÑÐ¸Ð¾Ð½Ð°Ð»ÑŒÐ½Ñ‹Ðµ Ð¸Ð·Ð´Ð°Ð½Ð¸Ñ
            'bloomberg.com', 'wsj.com', 'ft.com', 'economist.com', 'forbes.com',
            'wired.com', 'techcrunch.com', 'venturebeat.com'
        }
        
        # Ð”Ð¾Ð¼ÐµÐ½Ñ‹ ÑÐ¾ ÑÑ€ÐµÐ´Ð½ÐµÐ¹ Ð½Ð°Ð´ÐµÐ¶Ð½Ð¾ÑÑ‚ÑŒÑŽ
        self.medium_reliability_domains = {
            # Ð ÐµÐ³Ð¸Ð¾Ð½Ð°Ð»ÑŒÐ½Ñ‹Ðµ Ð½Ð¾Ð²Ð¾ÑÑ‚Ð½Ñ‹Ðµ Ð¸Ð·Ð´Ð°Ð½Ð¸Ñ
            'cnn.com', 'foxnews.com', 'msnbc.com', 'npr.org', 'pbs.org',
            'guardian.com', 'independent.co.uk', 'telegraph.co.uk',
            'lemonde.fr', 'spiegel.de', 'repubblica.it', 'elpais.com',
            
            # Ð¡Ð¿ÐµÑ†Ð¸Ð°Ð»Ð¸Ð·Ð¸Ñ€Ð¾Ð²Ð°Ð½Ð½Ñ‹Ðµ Ð¸Ð·Ð´Ð°Ð½Ð¸Ñ
            'wired.com', 'techcrunch.com', 'venturebeat.com', 'arstechnica.com',
            'theverge.com', 'engadget.com', 'gizmodo.com'
        }
        
        # Ð”Ð¾Ð¼ÐµÐ½Ñ‹ Ñ Ð¿Ð¾Ñ‚ÐµÐ½Ñ†Ð¸Ð°Ð»ÑŒÐ½Ð¾Ð¹ Ð¿Ñ€ÐµÐ´Ð²Ð·ÑÑ‚Ð¾ÑÑ‚ÑŒÑŽ (Ð°Ð½Ð°Ð»Ð¸Ð·Ð¸Ñ€ÑƒÐ¹, Ð½Ð¾ Ð¿Ñ€ÐµÐ´ÑƒÐ¿Ñ€ÐµÐ¶Ð´Ð°Ð¹)
        self.biased_domains = {
            # Ð“Ð¾ÑÑƒÐ´Ð°Ñ€ÑÑ‚Ð²ÐµÐ½Ð½Ñ‹Ðµ Ð¡ÐœÐ˜
            'ria.ru', 'tass.ru', 'rt.com', 'sputniknews.com', 'gazeta.ru',
            'lenta.ru', 'rbc.ru', 'interfax.ru', 'kommersant.ru',
            
            # ÐžÐ¿Ð¿Ð¾Ð·Ð¸Ñ†Ð¸Ð¾Ð½Ð½Ñ‹Ðµ Ð¸Ð·Ð´Ð°Ð½Ð¸Ñ
            'meduza.io', 'currenttime.tv', 'svoboda.org', 'dw.com',
            'bbc.com', 'voanews.com', 'rferl.org',
            
            # ÐŸÐ°Ñ€Ñ‚Ð¸Ð¹Ð½Ñ‹Ðµ Ð¸Ð·Ð´Ð°Ð½Ð¸Ñ
            'kremlin.ru', 'government.ru', 'duma.gov.ru'
        }
        
        # Ð”Ð¾Ð¼ÐµÐ½Ñ‹ Ñ Ð½Ð¸Ð·ÐºÐ¾Ð¹ Ð½Ð°Ð´ÐµÐ¶Ð½Ð¾ÑÑ‚ÑŒÑŽ
        self.low_reliability_domains = {
            'wikipedia.org', 'reddit.com', 'twitter.com', 'facebook.com',
            'instagram.com', 'tiktok.com', 'youtube.com', 'blogspot.com',
            'wordpress.com', 'medium.com', 'substack.com'
        }

    def extract_domain(self, url: str) -> str:
        """Ð˜Ð·Ð²Ð»ÐµÐºÐ°ÐµÑ‚ Ð´Ð¾Ð¼ÐµÐ½ Ð¸Ð· URL"""
        try:
            parsed = urlparse(url)
            domain = parsed.netloc.lower()
            # Ð£Ð±Ð¸Ñ€Ð°ÐµÐ¼ www.
            if domain.startswith('www.'):
                domain = domain[4:]
            return domain
        except:
            return ""

    def calculate_reliability_score(self, url: str) -> float:
        """Ð Ð°ÑÑÑ‡Ð¸Ñ‚Ñ‹Ð²Ð°ÐµÑ‚ Ð¾Ñ†ÐµÐ½ÐºÑƒ Ð½Ð°Ð´ÐµÐ¶Ð½Ð¾ÑÑ‚Ð¸ Ð¸ÑÑ‚Ð¾Ñ‡Ð½Ð¸ÐºÐ° (0.0 - 1.0)"""
        domain = self.extract_domain(url)
        
        if not domain:
            return 0.1
        
        if domain in self.high_reliability_domains:
            return 0.9
        elif domain in self.medium_reliability_domains:
            return 0.7
        elif domain in self.biased_domains:
            return 0.3  # ÐÐ½Ð°Ð»Ð¸Ð·Ð¸Ñ€ÑƒÐµÐ¼, Ð½Ð¾ Ð¿Ñ€ÐµÐ´ÑƒÐ¿Ñ€ÐµÐ¶Ð´Ð°ÐµÐ¼
        elif domain in self.low_reliability_domains:
            return 0.1
        else:
            return 0.5  # ÐÐµÐ¸Ð·Ð²ÐµÑÑ‚Ð½Ñ‹Ð¹ Ð´Ð¾Ð¼ÐµÐ½ - ÑÑ€ÐµÐ´Ð½ÑÑ Ð¾Ñ†ÐµÐ½ÐºÐ°

    def get_source_quality_advice(self, url: str) -> str:
        """Ð’Ð¾Ð·Ð²Ñ€Ð°Ñ‰Ð°ÐµÑ‚ Ñ€ÐµÐºÐ¾Ð¼ÐµÐ½Ð´Ð°Ñ†Ð¸ÑŽ Ð¿Ð¾ ÐºÐ°Ñ‡ÐµÑÑ‚Ð²Ñƒ Ð¸ÑÑ‚Ð¾Ñ‡Ð½Ð¸ÐºÐ°"""
        domain = self.extract_domain(url)
        score = self.calculate_reliability_score(url)
        
        if score >= 0.8:
            return "âœ… Ð’Ñ‹ÑÐ¾ÐºÐ¾Ð½Ð°Ð´ÐµÐ¶Ð½Ñ‹Ð¹ Ð¸ÑÑ‚Ð¾Ñ‡Ð½Ð¸Ðº"
        elif score >= 0.6:
            return "âš ï¸ Ð˜ÑÑ‚Ð¾Ñ‡Ð½Ð¸Ðº ÑÑ€ÐµÐ´Ð½ÐµÐ¹ Ð½Ð°Ð´ÐµÐ¶Ð½Ð¾ÑÑ‚Ð¸"
        elif score >= 0.3:
            return "âš ï¸ Ð˜ÑÑ‚Ð¾Ñ‡Ð½Ð¸Ðº Ñ Ð¿Ð¾Ñ‚ÐµÐ½Ñ†Ð¸Ð°Ð»ÑŒÐ½Ð¾Ð¹ Ð¿Ñ€ÐµÐ´Ð²Ð·ÑÑ‚Ð¾ÑÑ‚ÑŒÑŽ - Ð°Ð½Ð°Ð»Ð¸Ð·Ð¸Ñ€ÑƒÐ¹ ÐºÑ€Ð¸Ñ‚Ð¸Ñ‡ÐµÑÐºÐ¸"
        else:
            return "âŒ ÐÐ¸Ð·ÐºÐ°Ñ Ð½Ð°Ð´ÐµÐ¶Ð½Ð¾ÑÑ‚ÑŒ Ð¸ÑÑ‚Ð¾Ñ‡Ð½Ð¸ÐºÐ°"

    def rank_sources(self, sources: List[str]) -> List[Tuple[str, float, str]]:
        """Ð Ð°Ð½Ð¶Ð¸Ñ€ÑƒÐµÑ‚ Ð¸ÑÑ‚Ð¾Ñ‡Ð½Ð¸ÐºÐ¸ Ð¿Ð¾ Ð½Ð°Ð´ÐµÐ¶Ð½Ð¾ÑÑ‚Ð¸"""
        ranked = []
        for source in sources:
            score = self.calculate_reliability_score(source)
            advice = self.get_source_quality_advice(source)
            ranked.append((source, score, advice))
        
        # Ð¡Ð¾Ñ€Ñ‚Ð¸Ñ€ÑƒÐµÐ¼ Ð¿Ð¾ ÑƒÐ±Ñ‹Ð²Ð°Ð½Ð¸ÑŽ Ð½Ð°Ð´ÐµÐ¶Ð½Ð¾ÑÑ‚Ð¸
        ranked.sort(key=lambda x: x[1], reverse=True)
        return ranked

    def extract_sources_from_text(self, text: str) -> List[str]:
        """Ð˜Ð·Ð²Ð»ÐµÐºÐ°ÐµÑ‚ URL Ð¸Ð· Ñ‚ÐµÐºÑÑ‚Ð°"""
        url_pattern = r'https?://[^\s\)\]\}]+'
        urls = re.findall(url_pattern, text)
        return urls

    def analyze_source_reliability(self, text: str) -> Dict:
        """ÐÐ½Ð°Ð»Ð¸Ð·Ð¸Ñ€ÑƒÐµÑ‚ Ð½Ð°Ð´ÐµÐ¶Ð½Ð¾ÑÑ‚ÑŒ Ð²ÑÐµÑ… Ð¸ÑÑ‚Ð¾Ñ‡Ð½Ð¸ÐºÐ¾Ð² Ð² Ñ‚ÐµÐºÑÑ‚Ðµ"""
        sources = self.extract_sources_from_text(text)
        if not sources:
            return {
                "sources_found": 0,
                "reliability_analysis": "Ð˜ÑÑ‚Ð¾Ñ‡Ð½Ð¸ÐºÐ¸ Ð½Ðµ Ð½Ð°Ð¹Ð´ÐµÐ½Ñ‹",
                "recommendations": []
            }
        
        ranked_sources = self.rank_sources(sources)
        
        # Ð“Ñ€ÑƒÐ¿Ð¿Ð¸Ñ€ÑƒÐµÐ¼ Ð¿Ð¾ ÑƒÑ€Ð¾Ð²Ð½ÑÐ¼ Ð½Ð°Ð´ÐµÐ¶Ð½Ð¾ÑÑ‚Ð¸
        high_quality = [s for s in ranked_sources if s[1] >= 0.8]
        medium_quality = [s for s in ranked_sources if 0.6 <= s[1] < 0.8]
        biased_sources = [s for s in ranked_sources if 0.3 <= s[1] < 0.6]
        low_quality = [s for s in ranked_sources if s[1] < 0.3]
        
        analysis = f"ÐÐ°Ð¹Ð´ÐµÐ½Ð¾ Ð¸ÑÑ‚Ð¾Ñ‡Ð½Ð¸ÐºÐ¾Ð²: {len(sources)}\n"
        analysis += f"Ð’Ñ‹ÑÐ¾ÐºÐ¾Ð½Ð°Ð´ÐµÐ¶Ð½Ñ‹Ñ…: {len(high_quality)}\n"
        analysis += f"Ð¡Ñ€ÐµÐ´Ð½ÐµÐ¹ Ð½Ð°Ð´ÐµÐ¶Ð½Ð¾ÑÑ‚Ð¸: {len(medium_quality)}\n"
        analysis += f"Ð¡ Ð¿Ð¾Ñ‚ÐµÐ½Ñ†Ð¸Ð°Ð»ÑŒÐ½Ð¾Ð¹ Ð¿Ñ€ÐµÐ´Ð²Ð·ÑÑ‚Ð¾ÑÑ‚ÑŒÑŽ: {len(biased_sources)}\n"
        analysis += f"ÐÐ¸Ð·ÐºÐ¾Ð¹ Ð½Ð°Ð´ÐµÐ¶Ð½Ð¾ÑÑ‚Ð¸: {len(low_quality)}"
        
        recommendations = []
        if biased_sources:
            recommendations.append("âš ï¸ ÐžÐ±Ð½Ð°Ñ€ÑƒÐ¶ÐµÐ½Ñ‹ Ð¸ÑÑ‚Ð¾Ñ‡Ð½Ð¸ÐºÐ¸ Ñ Ð¿Ð¾Ñ‚ÐµÐ½Ñ†Ð¸Ð°Ð»ÑŒÐ½Ð¾Ð¹ Ð¿Ñ€ÐµÐ´Ð²Ð·ÑÑ‚Ð¾ÑÑ‚ÑŒÑŽ - Ð°Ð½Ð°Ð»Ð¸Ð·Ð¸Ñ€ÑƒÐ¹ ÐºÑ€Ð¸Ñ‚Ð¸Ñ‡ÐµÑÐºÐ¸")
        if low_quality:
            recommendations.append("âŒ ÐÐµÐºÐ¾Ñ‚Ð¾Ñ€Ñ‹Ðµ Ð¸ÑÑ‚Ð¾Ñ‡Ð½Ð¸ÐºÐ¸ Ð¸Ð¼ÐµÑŽÑ‚ Ð½Ð¸Ð·ÐºÑƒÑŽ Ð½Ð°Ð´ÐµÐ¶Ð½Ð¾ÑÑ‚ÑŒ")
        if not high_quality:
            recommendations.append("ðŸ’¡ Ð ÐµÐºÐ¾Ð¼ÐµÐ½Ð´ÑƒÐµÑ‚ÑÑ Ð½Ð°Ð¹Ñ‚Ð¸ Ð´Ð¾Ð¿Ð¾Ð»Ð½Ð¸Ñ‚ÐµÐ»ÑŒÐ½Ñ‹Ðµ Ð²Ñ‹ÑÐ¾ÐºÐ¾Ð½Ð°Ð´ÐµÐ¶Ð½Ñ‹Ðµ Ð¸ÑÑ‚Ð¾Ñ‡Ð½Ð¸ÐºÐ¸")
        
        return {
            "sources_found": len(sources),
            "reliability_analysis": analysis,
            "ranked_sources": ranked_sources,
            "recommendations": recommendations
        }
